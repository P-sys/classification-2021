{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Task_6.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFrqCTyKVXO2"
      },
      "source": [
        "## Данные\n",
        "\n",
        "Данные в [архиве](https://drive.google.com/file/d/15o7fdxTgndoy6K-e7g8g1M2-bOOwqZPl/view?usp=sharing). В нём два файла:\n",
        "- `news_train.txt` тестовое множество\n",
        "- `news_test.txt` тренировочное множество\n",
        "\n",
        "С некоторых новостных сайтов были загружены тексты новостей за период  несколько лет, причем каждая новость принаделжит к какой-то рубрике: `science`, `style`, `culture`, `life`, `economics`, `business`, `travel`, `forces`, `media`, `sport`.\n",
        "\n",
        "В каждой строке файла содержится метка рубрики, заголовок новостной статьи и сам текст статьи, например:\n",
        "\n",
        ">    **sport**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею разгромила чехов**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею крупно об...**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtyP-H_lOl1O",
        "outputId": "2d5f4dca-5a00-4a09-a549-d29256745c41"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWUW3U5eVXO5"
      },
      "source": [
        "# Задача\n",
        "\n",
        "1. Обработать данные, получив для каждого текста набор токенов\n",
        "Обработать токены с помощью (один вариант из трех):\n",
        "    - pymorphy2\n",
        "    - русского [snowball стеммера](https://www.nltk.org/howto/stem.html)\n",
        "    - [SentencePiece](https://github.com/google/sentencepiece) или [Huggingface Tokenizers](https://github.com/huggingface/tokenizers)\n",
        "    \n",
        "    \n",
        "2. Обучить word embeddings (fastText, word2vec, gloVe) на тренировочных данных. Можно использовать [gensim](https://radimrehurek.com/gensim/models/word2vec.html) . Продемонстрировать семантические ассоциации. \n",
        "\n",
        "3. Реализовать алгоритм классификации документа по категориям, посчитать точноть на тестовых данных, подобрать гиперпараметры. Метод векторизации выбрать произвольно - можно использовать $tf-idf$ с понижением размерности (см. scikit-learn), можно использовать обученные на предыдущем шаге векторные представления, можно использовать [предобученные модели](https://rusvectores.org/ru/models/). Имейте ввиду, что простое \"усреднение\" токенов в тексте скорее всего не даст положительных результатов. Нужно реализовать два алгоритмов из трех:\n",
        "     - SVM\n",
        "     - наивный байесовский классификатор\n",
        "     - логистическая регрессия\n",
        "    \n",
        "\n",
        "4.* Реализуйте классификацию с помощью нейросетевых моделей. Например [RuBERT](http://docs.deeppavlov.ai/en/master/features/models/bert.html) или [ELMo](https://rusvectores.org/ru/models/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQS093EHZHLk",
        "outputId": "5660f0c9-e8d9-48ad-f650-d3e50e3d3fb5"
      },
      "source": [
        "!pip install pymorphy2\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import pymorphy2"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.7/dist-packages (0.9.1)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (2.4.417127.4579844)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhXc5b1-XSv8"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3K8Klgneb43m"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG83J8Pxff9N"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKvceJQ1Z2FA"
      },
      "source": [
        "train_data = pd.read_csv('/content/drive/MyDrive/news_train.txt', sep=\"\\t\", header=None, names=['topic', 'headline', 'info'])\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/news_test.txt', sep=\"\\t\", header=None, names=['topic', 'headline', 'info'])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "gnBiHsveaj7f",
        "outputId": "acdb29d5-08f5-41d4-bb2c-1b81f5ee5154"
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>topic</th>\n",
              "      <th>headline</th>\n",
              "      <th>info</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sport</td>\n",
              "      <td>Овечкин пожертвовал детской хоккейной школе ав...</td>\n",
              "      <td>Нападающий «Вашингтон Кэпиталз» Александр Овеч...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>culture</td>\n",
              "      <td>Рекордно дорогую статую майя признали подделкой</td>\n",
              "      <td>Власти Мексики объявили подделкой статую майя,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>science</td>\n",
              "      <td>Samsung представила флагман в защищенном корпусе</td>\n",
              "      <td>Южнокорейская Samsung анонсировала защищенную ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sport</td>\n",
              "      <td>С футболиста «Спартака» сняли четырехматчевую ...</td>\n",
              "      <td>Контрольно-дисциплинарный комитет (КДК) РФС сн...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>media</td>\n",
              "      <td>Hopes &amp; Fears объединится с The Village</td>\n",
              "      <td>Интернет-издание Hopes &amp; Fears объявило о свое...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     topic  ...                                               info\n",
              "0    sport  ...  Нападающий «Вашингтон Кэпиталз» Александр Овеч...\n",
              "1  culture  ...  Власти Мексики объявили подделкой статую майя,...\n",
              "2  science  ...  Южнокорейская Samsung анонсировала защищенную ...\n",
              "3    sport  ...  Контрольно-дисциплинарный комитет (КДК) РФС сн...\n",
              "4    media  ...  Интернет-издание Hopes & Fears объявило о свое...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZrBOB90arAf"
      },
      "source": [
        "morph = pymorphy2.MorphAnalyzer()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKcCE-4-UKpH",
        "outputId": "bc4f022f-3d21-4e40-904a-4f22b3a5ce65"
      },
      "source": [
        "import string\n",
        "def remove_punctuation(text):\n",
        "    return \"\".join([ch if ch not in string.punctuation + '«' + '»' + '—' else ' ' for ch in text])\n",
        "\n",
        "def remove_numbers(text):\n",
        "    return ''.join([i if not i.isdigit() else ' ' for i in text])\n",
        "\n",
        "import re\n",
        "def remove_multiple_spaces(text):\n",
        "\treturn re.sub(r'\\s+', ' ', text, flags=re.I)\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "\n",
        "\n",
        "russian_stopwords = stopwords.words(\"russian\")\n",
        "russian_stopwords.extend(['…', '«', '»', '...'])\n",
        "\n",
        "def token(text):\n",
        "    word_list = text.strip().split()\n",
        "    res = []\n",
        "\n",
        "    for word in word_list:\n",
        "      p = morph.parse(word)[0]\n",
        "      res.append(p.normal_form)\n",
        "    \n",
        "    tokens = [token for token in res if token not in russian_stopwords and token != \" \"]\n",
        "    text = \" \".join(tokens)\n",
        "\n",
        "    return text"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3r3rtJeLx7f"
      },
      "source": [
        " for i in range(len(train_data['info'])):\n",
        "    st = remove_punctuation(train_data['info'][i])\n",
        "    st = remove_numbers(st)\n",
        "    st = remove_multiple_spaces(st)\n",
        "\n",
        "    st = token(st)\n",
        "\n",
        "    train_data['info'][i] = st"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AL1tGlVMaTu7"
      },
      "source": [
        " for i in range(len(test_data['info'])):\n",
        "    st = remove_punctuation(test_data['info'][i])\n",
        "    st = remove_numbers(st)\n",
        "    st = remove_multiple_spaces(st)\n",
        "\n",
        "    st = token(st)\n",
        "\n",
        "    test_data['info'][i] = st"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "UMrFTa2tvzBi",
        "outputId": "ca98b922-2f8b-4fdd-906c-2c39488207cc"
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>topic</th>\n",
              "      <th>headline</th>\n",
              "      <th>info</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sport</td>\n",
              "      <td>Овечкин пожертвовал детской хоккейной школе ав...</td>\n",
              "      <td>нападать вашингтон кэпиталзти александр овечки...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>culture</td>\n",
              "      <td>Рекордно дорогую статую майя признали подделкой</td>\n",
              "      <td>власть мексика объявить подделка статуя майя п...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>science</td>\n",
              "      <td>Samsung представила флагман в защищенном корпусе</td>\n",
              "      <td>южнокорейский samsung анонсировать защитить ве...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sport</td>\n",
              "      <td>С футболиста «Спартака» сняли четырехматчевую ...</td>\n",
              "      <td>контрольный дисциплинарный комитет кдк рфс сня...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>media</td>\n",
              "      <td>Hopes &amp; Fears объединится с The Village</td>\n",
              "      <td>интернет издание hopes fears объявить свой сли...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     topic  ...                                               info\n",
              "0    sport  ...  нападать вашингтон кэпиталзти александр овечки...\n",
              "1  culture  ...  власть мексика объявить подделка статуя майя п...\n",
              "2  science  ...  южнокорейский samsung анонсировать защитить ве...\n",
              "3    sport  ...  контрольный дисциплинарный комитет кдк рфс сня...\n",
              "4    media  ...  интернет издание hopes fears объявить свой сли...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOMLTVLvyrpM"
      },
      "source": [
        "train_data.to_csv( \"//content/drive/MyDrive/df_train.csv\", index=False, encoding='utf-8-sig')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BC0PyDTz4ZFR"
      },
      "source": [
        "test_data.to_csv( \"//content/drive/MyDrive/df_test.csv\", index=False, encoding='utf-8-sig')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3sLYdXeX67e"
      },
      "source": [
        "#Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sbWD0KhoJIV"
      },
      "source": [
        "!pip install gensim\n",
        "from gensim.models import Word2Vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhSq7P80wfHm"
      },
      "source": [
        "info_list = [i.split() for i in train_data['info']]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogHTI7zrRY7A"
      },
      "source": [
        "model = Word2Vec(sentences=info_list, min_count=1, workers=4)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vb9BTGzVRr6S",
        "outputId": "603f2134-4963-4cf3-8a4e-4eed8acbc30d"
      },
      "source": [
        "model.wv.most_similar('власть')"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('чиновник', 0.7827291488647461),\n",
              " ('сторона', 0.773669958114624),\n",
              " ('евросоюз', 0.7555724382400513),\n",
              " ('правительство', 0.7437057495117188),\n",
              " ('еврокомиссия', 0.7348254323005676),\n",
              " ('урегулировать', 0.7177960276603699),\n",
              " ('минюст', 0.7170125246047974),\n",
              " ('тегеран', 0.7161378264427185),\n",
              " ('киев', 0.7147305011749268),\n",
              " ('баньоля', 0.7086164355278015)]"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTDWXB_sXP0b"
      },
      "source": [
        "label_encoder = LabelEncoder()\n",
        "train_data['topic_target'] = label_encoder.fit_transform(train_data['topic'])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQlnPFFMbRvp"
      },
      "source": [
        "test_data['topic_target'] = label_encoder.transform(test_data['topic'])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhM_Y2vjYEUJ"
      },
      "source": [
        "#Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-d4WTeN1N7I"
      },
      "source": [
        "ngram_range = (1, 2)\n",
        "min_df = 10\n",
        "max_df = 1.\n",
        "max_features = 300"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_PiQXOPgXcR"
      },
      "source": [
        "tfidf = TfidfVectorizer(encoding = 'utf-8',\n",
        "                        ngram_range = ngram_range,\n",
        "                        stop_words = None,\n",
        "                        lowercase = False,\n",
        "                        max_df = max_df,\n",
        "                        min_df = min_df,\n",
        "                        max_features = max_features,\n",
        "                        norm = 'l2',\n",
        "                        sublinear_tf = True)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xA495kS57nZ"
      },
      "source": [
        "feature_train = tfidf.fit_transform(train_data['info']).toarray()\n",
        "labels_train = train_data['topic_target']\n",
        "\n",
        "feature_test = tfidf.transform(test_data['info']).toarray()\n",
        "labels_test = test_data['topic_target']"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzYdsGCfXxPL"
      },
      "source": [
        "#NB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDBqSs2elxec",
        "outputId": "ba4b414d-f69a-4d1c-8902-b8d51344d128"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "param_grid = {'alpha': [0.01, 0.1, 0.5, 1.0, 10.0]}\n",
        "model_nb = MultinomialNB()\n",
        "grid = GridSearchCV(model_nb, param_grid, cv=5, verbose=5)\n",
        "best = grid.fit(feature_train, labels_train)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
            "[CV 1/5] END ........................alpha=0.01;, score=0.738 total time=   0.0s\n",
            "[CV 2/5] END ........................alpha=0.01;, score=0.739 total time=   0.0s\n",
            "[CV 3/5] END ........................alpha=0.01;, score=0.741 total time=   0.0s\n",
            "[CV 4/5] END ........................alpha=0.01;, score=0.729 total time=   0.0s\n",
            "[CV 5/5] END ........................alpha=0.01;, score=0.735 total time=   0.0s\n",
            "[CV 1/5] END .........................alpha=0.1;, score=0.738 total time=   0.0s\n",
            "[CV 2/5] END .........................alpha=0.1;, score=0.739 total time=   0.0s\n",
            "[CV 3/5] END .........................alpha=0.1;, score=0.741 total time=   0.0s\n",
            "[CV 4/5] END .........................alpha=0.1;, score=0.729 total time=   0.0s\n",
            "[CV 5/5] END .........................alpha=0.1;, score=0.737 total time=   0.0s\n",
            "[CV 1/5] END .........................alpha=0.5;, score=0.737 total time=   0.0s\n",
            "[CV 2/5] END .........................alpha=0.5;, score=0.739 total time=   0.0s\n",
            "[CV 3/5] END .........................alpha=0.5;, score=0.740 total time=   0.0s\n",
            "[CV 4/5] END .........................alpha=0.5;, score=0.729 total time=   0.0s\n",
            "[CV 5/5] END .........................alpha=0.5;, score=0.736 total time=   0.0s\n",
            "[CV 1/5] END .........................alpha=1.0;, score=0.735 total time=   0.0s\n",
            "[CV 2/5] END .........................alpha=1.0;, score=0.737 total time=   0.0s\n",
            "[CV 3/5] END .........................alpha=1.0;, score=0.739 total time=   0.0s\n",
            "[CV 4/5] END .........................alpha=1.0;, score=0.728 total time=   0.0s\n",
            "[CV 5/5] END .........................alpha=1.0;, score=0.733 total time=   0.0s\n",
            "[CV 1/5] END ........................alpha=10.0;, score=0.721 total time=   0.0s\n",
            "[CV 2/5] END ........................alpha=10.0;, score=0.725 total time=   0.0s\n",
            "[CV 3/5] END ........................alpha=10.0;, score=0.729 total time=   0.0s\n",
            "[CV 4/5] END ........................alpha=10.0;, score=0.716 total time=   0.0s\n",
            "[CV 5/5] END ........................alpha=10.0;, score=0.716 total time=   0.0s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmM4_t1U2j_o",
        "outputId": "c86ea4a2-f5e5-4006-df96-f6e5b99f7264"
      },
      "source": [
        "model_predictions_nb = best.predict(feature_test)\n",
        "print(accuracy_score(labels_test, model_predictions_nb))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.751\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAlpHdl5Xth4"
      },
      "source": [
        "#SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3P0JlraG33E"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid']}\n",
        "model_sv = SVC()\n",
        "grid_1 = GridSearchCV(model_sv, param_grid, cv=3, verbose=2)\n",
        "bestF = grid_1.fit(feature_train, labels_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egdGysHPBd4v",
        "outputId": "6fce48b8-5029-4625-dba9-3f43595e7383"
      },
      "source": [
        "model_predictions_svm = bestF.predict(feature_test)\n",
        "print(accuracy_score(labels_test, model_predictions_svm))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8156666666666667\n"
          ]
        }
      ]
    }
  ]
}